{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project: A Simple Search Engine from Scratch\n",
    "The project was focused on the study of IR fundamental algorithms.\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Our projected consisted in analyze and compare some algorithm that are considered fundamental in IR.\n",
    "The algorithms we analyzed were, \n",
    "\n",
    "- Vector Space Model \n",
    "- Language Model with Jelineck-Mercer\n",
    "- Language Model with Dirichlet \n",
    "- Relevance Model 3\n",
    "\n",
    "We are going to present the implementation we made for these algorithm and some experiments and analysis we did to compare the different algorithm's performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index\n",
    "\n",
    "Both these algorithm can be implemented in two ways.\n",
    "One is using matrixes, which is computationally heavier since it computes the similarity for all documents in the collection. \n",
    "The other way is using the inverted index, which only computes the similarity for the documents containing the query words.\n",
    "\n",
    "The only model we could implement the inverted index was the Vector Space Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Vector Space Model (VSM)\n",
    "\n",
    "In the vector space model, documents are represented as a vector $d_j=(w_{d_j,1},w_{d_j,2}, ..., w_{d_j,n})$ of $n$ word frequencies -- most of the words are equal to 0. Queries are also represented as a vector of words $q_j=(w_{q_j,1},w_{q_j,2}, ..., w_{q_j,n})$. In the vector space model, each document word is weighted by their *tf-idf*\n",
    "\n",
    "$${tf-idf} = tf*\\frac{|D|}{log (df(w_a))}$$\n",
    "\n",
    "The vector space model is based on the cosine similarity, which measures the angle between the two vectors in the 1-unit sphere:\n",
    "\n",
    "$$cos(q,d) = \\frac{\\sum_t q_t\\cdot d_t}{\\sqrt{\\sum_t q^2_t}\\cdot \\sqrt{\\sum_t d^2_t }}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Language Model with Dirichlet (LMD)\n",
    "\n",
    "The Dirichlet smoothing model uses a mixture of frequencies between the term document frequencies $f_{t,c}$ and the term corpus frequency $\\mu \\cdot M_c(t)$:\n",
    "\n",
    "$$p(t|M_d, M_c)= \\frac{f_{t,d}+\\mu \\cdot M_c(t)}{|d| + \\mu}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Language Model with Jelineck-Mercer (LMJM)\n",
    "\n",
    "The Jelineck-Mercer smoothing model uses a mixture of probabilities between the document model $M_d$ and the corpus model $M_c$:\n",
    "\n",
    "$$p(q|d,C)= \\lambda \\cdot p(q|M_d) + (1-\\lambda)\\cdot p(q|M_c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Relevance Model 3 (RM3)\n",
    "\n",
    "The Relevance models aim to estimate the relevance of each word in a query. The model expand the initial query with new terms so then it can compute its probability and the words that have a probability below a threshold are zeroed. With that it possible to produce a new query that will bring more accurate values according with the user feedback.\n",
    "\n",
    "The Relevance Model 3 assumes the words in the original query are independent:\n",
    "\n",
    "$$p_{rm1}(w|Q)= \\sum_{M_d \\in \\Theta} p(w|M_d)p(M_d)\\prod_{i=1}^{m} p(q_i|M_d)$$\n",
    "\n",
    "The final relevance model becomes:\n",
    "\n",
    "$$p_{rm3}(w|M'_Q)= (1-\\alpha) \\cdot p(w|M_Q) + \\alpha \\cdot p_{rm1}(w|Q)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
